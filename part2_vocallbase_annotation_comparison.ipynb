{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95ed31c-aad0-481a-8637-50f05093f19f",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Labeling Animal Vocalizations with VoCallBase and Comparing Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c680151-3890-4fb5-bb0f-cba73712a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from label_comparison import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5dd6ae-8e7d-4483-9fce-1573dfe44da3",
   "metadata": {},
   "source": [
    "## 2a) Labeling Animal Vocalizations\n",
    "## Go to Xeno-canto and listen to some clips for the Screaming Piha - https://xeno-canto.org/species/Lipaugus-vociferans\n",
    "## and the Bright-rumped Attila - https://xeno-canto.org/species/Attila-spadiceus\n",
    "## Then go to this VoCallBase project google sheet - https://docs.google.com/spreadsheets/d/1mNDR1e7smgUdmJKE5pga5wWUcUrYN33LT6xjjxPPh-M/edit?usp=sharing\n",
    "## And select the link for the user corresponding to your given number\n",
    "## In your VoCallBase page add in Species [attila, piha, insect, bird], add a cluster to the species with the same name i.e. (species=piha, cluster=piha)\n",
    "\n",
    "## Now, try and label everything in the provided audio clip based on the provided list of species and from listening to Xeno-canto clips "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559fcd7-d82e-4901-b8bf-62ce9da1c0ae",
   "metadata": {},
   "source": [
    "# Loading in the Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3f5b2-817e-4730-8554-cd46cbc9564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for what is considered ground truth (often from an expert in a species or a field site)\n",
    "gt_csv_path = \"piha_attila_nccr_vocallbase_practical_annotations.csv\"\n",
    "gt_df = pd.read_csv(gt_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61065216-00f6-4e40-8eb6-20d345b57c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for what we want to compare (often another researcher or an AI model such as WhisperSeg)\n",
    "csv_path = \"student_annotations.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1e36a-f15a-4f81-8c48-530941fa05ec",
   "metadata": {},
   "source": [
    "## 2b) Comparing Annotations -- What leads to agreements/disagreements?\n",
    "## This question is a bit more open-ended, if you look into the label_comparison.py file, you will find some functions that can help you with comparing your annotations to those provided or to those of one of your peers. I'll prompt a couple of ideas below with some details on the helper functions - \n",
    "\n",
    "### spectrogram_comparison(clip_path, df1, df2, target_cluster) -- do you see any difference between clusters?\n",
    "\n",
    "### segment_score(alt_df, gt_df, target_cluster, tolerance) -- What difference do you see between precision and recall? Any difference between clusters?\n",
    "### frame_score(alt_df, gt_df, target_cluster, tolerance) -- Has the same interface as segment_score.\n",
    "\n",
    "### do you see any difference between frame and segment metrics? How does the tolerance variable affect the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb7be72-04da-4627-9374-be92c6b1dc20",
   "metadata": {},
   "source": [
    "## Initial F1 Comparisons\n",
    "### Refer to WhisperSeg framewise annotation comparison framework for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090a123-59bc-4278-afaf-a718b1805107",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_results = frame_score(gt_df, df)\n",
    "print(\"Aggregate frame-wise F1: \", frame_results[\"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf949bb-df69-4c59-b251-a9bfd4a1b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_clip_path = \"lipaugus101_DKMT1hnl.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ec2c6-ec44-4fee-bbed-796165250df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sanity check that you filled in the cluster/species names correctly\")\n",
    "gt_clusters = set(gt_df[\"clustername\"])\n",
    "print(\"Ground Truth Clusters\", gt_clusters)\n",
    "alt_clusters = set(gt_df[\"clustername\"])\n",
    "print(\"Alternative Clusters\", alt_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
